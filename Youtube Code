#Code for Extracting all YouTube Videos, Metadata, and Comment Threads containing 'Antonine Wall'



#Install Packages, set Authkey
install.packages("tuber")
library(tuber)

#set Authkey

yt_oauth("1043272008101-8s368d6a9b52869v8bb90vq8q6d2ghir.apps.googleusercontent.com","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", scope = "ssl")

Antonine_Wall <-yt_search("Antonine Wall", published_after = "2005-02-14T00:00:00Z", published_before = "2022-11-09T00:00:00Z")

saveRDS(Antonine_Wall, file = "Youtube_Antonine_Wall.RDS") 


VidIDsAntonineWall <- data.frame(Youtube_Antonine_Wall$video_id)
VidIDsAntonineWallList <- Youtube_Antonine_Wall$video_id

# Initialize an empty dataframe to store the stats
AntoninWallVidStats <- data.frame()

# Loop through each video ID and get stats
for (video_id in VidIDsAntonineWallList) {
  stats <- get_stats(video_id = video_id)
  # get_stats returns a dataframe, append it to the existing dataframe
  AntoninWallVidStats <- rbind(AntoninWallVidStats, stats)
}

##Then make it Joinable
AntoninWallVidStats <- AntoninWallVidStats %>%
rename(video_id = id)

Stats <- toJSON(AntoninWallVidStats, pretty = TRUE)
write(Stats, "Stats.json")



# Initialize an empty dataframe to store the comments for each video
AllComments <- data.frame()
for (video_id in VidIDsAntonineWallList) {
  comments <- get_all_comments(video_id = video_id)
  # Add a column for video ID to the comments dataframe
  comments$VideoID <- video_id



# Combine the comments with the main dataframe
  AllComments <- rbind(AllComments, comments)
}
Comments <- toJSON(AllComments, pretty = TRUE)
write(Comments, "Comments.json")


# combining
comments_json <- fromJSON("Comments.json")
stats_json <- fromJSON("Stats.json")
videos_json <- fromJSON("Youtube_Antonine_Wall.json")

comments_json$videoId <- as.character(comments_json$videoId)
videos_json$video_id <- as.character(videos_json$video_id)

videos_with_comments <- videos_json %>%
  mutate(
    comments = lapply(video_id, function(v_id) {
      comments_subset <- subset(comments_json, videoId == v_id)
      if (nrow(comments_subset) > 0) {
        comments_subset[, c("id", "textDisplay", "authorDisplayName", "authorProfileImageUrl", "likeCount", "publishedAt")]
      } else {
        data.frame()
      }
    })
  )
  
  
  stats_json$id <- as.character(stats_json$id)

videos_with_comments_and_stats <- left_join(videos_with_comments, stats_json, by = c("video_id" = "id"))

final_json <- toJSON(videos_with_comments_and_stats, pretty = TRUE)
write(final_json, "Youtube_data.json")

#Data was sybsequently vetted in and irrelevent rows and columns removed


#code additions and anonymisation took place in python

#adding code
import json

# Function to read a JSON file
def read_json_file(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data

# Function to add CommentCode within each comment
def add_comment_codes(json_data):
    for item in json_data:
        base_code = item['Code']
        for i, comment in enumerate(item['comments']):
            comment['CommentCode'] = f"{base_code}.{i+1}"
    return json_data

# Function to write JSON data to a file
def write_json_file(data, file_path):
    with open(file_path, 'w') as file:
        json.dump(data, file, indent=4)

file_path = 'YouTube_Data.json'
modified_file_path = 'Updated_YouTube_Data.json'
json_data = read_json_file(file_path)
modified_json_data = add_comment_codes(json_data)
write_json_file(modified_json_data, modified_file_path)


# Anonymising
for video_list in modified_json_data:
    for video in video_list:
        # Replace 'channelTitle' usernames
        channel_title = video.get('channelTitle', '')
        if channel_title and channel_title not in username_codes:
            username_codes[channel_title] = f'YT.Auth.{code_counter}'
            code_counter += 1
        video['channelTitle'] = username_codes.get(channel_title, '')

        # Replace 'authorDisplayName' in comments
        for comment in video.get('comments', []):
            author_display_name = comment.get('authorDisplayName', '')
            if author_display_name and author_display_name not in username_codes:
                username_codes[author_display_name] = f'YT.Auth.{code_counter}'
                code_counter += 1
            comment['authorDisplayName'] = username_codes.get(author_display_name, '')

# Save the modified data back to a file
New_file_path = 'Modified_YouTube_Data.json'
with open(new_file_path, 'w', encoding='utf-8') as file:
    json.dump(modified_json_data, file, ensure_ascii=False, indent=4)

new_file_path

# data then uploaded manually to mongodb


