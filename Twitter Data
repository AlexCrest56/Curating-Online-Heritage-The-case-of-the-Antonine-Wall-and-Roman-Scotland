install.packages("academictwitteR")
library(academictwitteR)
library(jsonlite)


# Set bearer token. This is the one copied from the Developer Academic twitter portal
bearer_token <-"AAAAAAAAAAAAAAAAAAAAABskRAEAAAAAF4JQUVo%2FTqjlVGBHCOF8a8Jowyc%3DPuUfLzBuFURNLLDiap26HlMPhmFd4FEporX2oC7ejCTZLvSBfm"


#Tweets
Data_Antonine_Wall <- get_all_tweets(
"antonine wall",
"2006-07-15T00:00:00Z",
"2022-11-09T00:00:00Z",
bearer_token,
bind_tweets = TRUE,
n = 1000000)


#USER Profiles
# Create a list of each of the dataframe's author_id's, and run a line of code
#for each
Users_List_Antonine_Wall <- (Data_Antonine_Wall$author_id)
Users_Data_Antonine_Wall <- get_user_profile(Users_List_Antonine_Wall, bearer_token)

#Merging these with tweet dataframes
#First, rename several columns in User_Data frames that have the same name as tweet data, but refer to different things, and to allow for merging.
install.packages("tidyverse")
library(tidyverse)
Users_Data_Antonine_Wall_Joinable <- Users_Data_Antonine_Wall %>%
rename(author_id = id)
Users_Data_Antonine_Wall_Joinable <- Users_Data_Antonine_Wall_Joinable %>%
rename(account_created = created_at)

CombData_Antonine_Wall <- full_join(Data_Antonine_Wall, Users_Data_Antonine_Wall_Joinable, by = 'author_id')
CombData_Antonine_Wall <- CombData_Antonine_Wall[!duplicated(CombData_Antonine_Wall$id),]

#Save data
saveRDS(CombData_Antonine_Wall, file = "CombData_Antonine_Wall.RDS")

#Write as json
CombData_Antonine_Wall <- toJSON(CombData_Antonine_Wall, pretty = TRUE)

write(CombData_Antonine_Wall, "D:/Users/Alexander Hiscock/Documents/Archaeology Research/Archaeology/PhD\Thesis (Sections, Work, Data)/CODE/DATA/Twitter/Data Collection/FOR COPYING ONLY Twitter. Combined Tweets Master Document/CombData_Antonine_Wall.json")

Twitter_data <- fromJSON("CombData_Antonine_Wall.json")
library(dplyr)

#I don't need these columns
possibly_sensitive
edit_history_tweet_ids
protected
profile_image_url


# delete columns
Twitter_data <- Twitter_data %>% select(-possibly_sensitive, -edit_history_tweet_ids, -protected, -profile_image_url)
#save
Twitter_data <- toJSON(Twitter_data, pretty = TRUE)
write_json(Twitter_data, path = "Twitter_data.json", pretty = TRUE)


#saving into smaller files that are more easily handled
# Calculate the number of rows in each group
group_size <- 3000

# Create a grouping variable
group <- ceiling(seq_len(nrow(Twitter_data)) / group_size)

# Split the dataframe into a list of dataframes
list_of_dfs <- split(Twitter_data, group)



small_df_1 <- toJSON(small_df_1, file = "small_df_1.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_1 , path = "small_df_1.json", pretty = TRUE)

small_df_2 <- toJSON(small_df_2, file = "small_df_2.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_2 , path = "small_df_2.json", pretty = TRUE)

small_df_3 <- toJSON(small_df_3, file = "small_df_3.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_3 , path = "small_df_3.json", pretty = TRUE)

small_df_4 <- toJSON(small_df_4, file = "small_df_4.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_4 , path = "small_df_4.json", pretty = TRUE)

small_df_5 <- toJSON(small_df_5, file = "small_df_5.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_5 , path = "small_df_5.json", pretty = TRUE)

small_df_6 <- toJSON(small_df_6, file = "small_df_6.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_6 , path = "small_df_6.json", pretty = TRUE)

small_df_7 <- toJSON(small_df_7, file = "small_df_7.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_7 , path = "small_df_7.json", pretty = TRUE)

small_df_8 <- toJSON(small_df_8, file = "small_df_8.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_8 , path = "small_df_8.json", pretty = TRUE)

small_df_9 <- toJSON(small_df_9, file = "small_df_9.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_9 , path = "small_df_9.json", pretty = TRUE)

small_df_10 <- toJSON(small_df_10, file = "small_df_10.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_10 , path = "small_df_10.json", pretty = TRUE)

small_df_11 <- toJSON(small_df_11, file = "small_df_11.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_11 , path = "small_df_11.json", pretty = TRUE)

small_df_12 <- toJSON(small_df_12, file = "small_df_12.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_12 , path = "small_df_12.json", pretty = TRUE)

small_df_13 <- toJSON(small_df_13, file = "small_df_13.json", pretty = TRUE, auto_unbox = TRUE)
write_json(small_df_13 , path = "small_df_13.json", pretty = TRUE)










# Load the jsonlite package
library(jsonlite)

# Assuming 'Twitter_data' is already loaded and is a data frame
# Check if 'Twitter_data' is a data frame
if (!is.data.frame(Twitter_data)) {
  stop("Twitter_data is not a data frame.")
}

# Set the number of rows per chunk
rows_per_chunk <- 3000

# Calculate the number of chunks needed
num_chunks <- ceiling(nrow(Twitter_data) / rows_per_chunk)

# Split and write each chunk
for (i in 1:num_chunks) {
  # Calculate row indices for this chunk
  start_row <- (i - 1) * rows_per_chunk + 1
  end_row <- min(i * rows_per_chunk, nrow(Twitter_data))
  
  # Extract the chunk
  chunk <- Twitter_data[start_row:end_row, ]
  
  # Generate a unique filename for each chunk
  file_name <- sprintf("Twitter_data_chunk_%03d.json", i)
  
  # Write the chunk to the new JSON file
  toJSON(chunk, file = file_name, pretty = TRUE, auto_unbox = TRUE)
  
  # Print the filename for debugging
  print(paste("Created file:", file_name))
}

# Print completion message
cat("Splitting completed into", num_chunks, "files.")

# List of Twitter data chunk names
twitter_data_chunks <- c("Twitter_Data_1", "Twitter_Data_2", "Twitter_Data_3", 
                         "Twitter_Data_4", "Twitter_Data_5", "Twitter_Data_6", 
                         "Twitter_Data_7", "Twitter_Data_8", "Twitter_Data_9", 
                         "Twitter_Data_10", "Twitter_Data_11", "Twitter_Data_12", 
                         "Twitter_Data_13")

# Iterate over each chunk and write to a separate JSON file
for(chunk in twitter_data_chunks) {
  chunk_path <- paste0(chunk, ".json")
  write_json(get(chunk), path = chunk_path, pretty = TRUE)
}