# Files were scraped using apify scraper and saved into directory. All posts with #antoninewall were scraped, including comments
#All profile data from the authors of these posts was also scraped
library(dplyr)
library(jsonlite)

#get posts from hashtag search results
Posts <- fromJSON("dataset_instagram-hashtag-scraper_2023-12-14_02-49-11-550.json")

#get comments from search results
Comments <- fromJSON("dataset_instagram-comment-scraper_2023-12-23_21-25-07-196.json")

#clean duplicate posts
Posts <- Posts %>% 
  distinct(url, .keep_all = TRUE)

#Remove Posts columns we don't need. These are:
##inputUrl
##type
##firstComment
##latestComments
##dimensionsHeight
##dimensionsWidth
##displayUrl
##images
##alt
##childPosts
##ownerFullName
##fullName
##locationId
##videoUrl
##productType

Posts <- Posts %>% 
  select(-inputUrl, -type,-firstComment,-latestComments,-dimensionsHeight,-dimensionsWidth,-displayUrl,-images,-alt,-childPosts,-ownerFullName,-fullName,-locationId,-videoUrl,-productType)

#Remove Comments columns we don't need. these are:
##ownerProfilePicURL

Comments <- Comments %>% 
  select(-ownerProfilePicUrl)

# Grouping Comments in lists and attaching to posts
# Ensure that the URLs are in the same format before merging
Comments$postUrl <- as.character(Comments$postUrl)
Posts$url <- as.character(Posts$url)

# Group comments by post URL and create a list of comments for each post
grouped_comments <- Comments %>%
  group_by(postUrl) %>%
  summarise(comments = list(data.frame(id, text, ownerUsername, timestamp, likesCount)))
  
# Join the grouped comments with the posts
Posts__With_Comments <- left_join(Posts, grouped_comments, by = c("url" = "postUrl"))

# Now add and combine profile data
Profiles1 <- fromJSON("dataset_instagram-profile-scraper_2023-12-14_15-40-04-317.json")
Profiles2 <- fromJSON("dataset_instagram-profile-scraper_2023-12-14_15-52-43-348.json")
Profiles3 <- fromJSON("dataset_instagram-profile-scraper_2023-12-14_15-59-34-696.json")
Profiles4 <- fromJSON("dataset_instagram-profile-scraper_2023-12-14_16-03-27-292.json")

# Merge the dataframes
Profiles <- rbind(Profiles1, Profiles2, Profiles3, Profiles4)

#Remove Profile columns we don't need. these are:
##externalUrlShimmed
##hasChannel
##joinedRecently
##businessCategoryName
##private
##verified
##profilePicUrlHD
##igtvVideoCount
##relatedProfiles
##latestPosts

Profiles <- Profiles %>% 
  select(-externalUrlShimmed, -hasChannel, -joinedRecently, -businessCategoryName, -private, -verified, -profilePicUrlHD, -igtvVideoCount, -relatedProfiles, -latestPosts)

#merge with comments
Posts__With_Comments_and_Profiles <- left_join(Posts__With_Comments, Profiles, by = c("ownerId" = "id"))

#Delete duplicates
Posts__With_Comments_and_Profiles <- Posts__With_Comments_and_Profiles %>% distinct(Posts__With_Comments_and_Profiles$id, .keep_all = TRUE)

#save as json
write_json(Posts__With_Comments_and_Profiles, path = "Instagram_Data_Clean.json", pretty = TRUE)


# adding codes and anonymisation were completed in python
import json

# Load the JSON file
file_path = 'Instagram_Data_Clean.json'

with open(file_path, 'r') as file:
    data = json.load(file)

# Iterate through the data and add the 'Code' and Comment 'Code' fields
for i, item in enumerate(data):
    # Adding 'Code'
    item['Code'] = f'IG.{i + 1}'

    # Check if 'comments' is a list and add 'Code' to each comment
    if isinstance(item.get('comments'), list):
        for j, comment in enumerate(item['comments']):
            comment['Code'] = f'{item["Code"]}.{j + 1}'

# Save the modified data back to the file
modified_file_path = 'Instagram_Data_Modified.json'
with open(modified_file_path, 'w') as file:
    json.dump(data, file, indent=4)

#data was then manually uploaded to mongodb
